{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('515K hotel dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing value: Drop the reviews with missing value directly.\n",
    "data.dropna(inplace = True)\n",
    "data.drop('Unnamed: 0', 1, inplace =True)\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-side Reviews: Remove the review with \"No Negative\"/\"No Positive\". \n",
    "data['NegativeReview'].replace('No Negative', \"\", inplace = True)\n",
    "data['PositiveReview'].replace('No Positive', \"\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine: Positive and negative reviews would be treated as only a review, and in addition : lower the case. \n",
    "corpus = data.NegativeReview + data.PositiveReview\n",
    "data.insert(0, \"Review\", corpus.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欲刪除的標點符號\n",
    "import re\n",
    "remove1 = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'  \n",
    "\n",
    "for i in range(len(data['Review'])):\n",
    "    print(i)\n",
    "    data['Review'][i] = re.sub(remove1,\"\",data['Review'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization: Conduct the work_tokenize first. (sent_tokenizing is more complicated in this case.)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  \n",
    "word_tokenized = data.Review.apply(word_tokenize)\n",
    "data.insert(0,\"WordToken\", word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly, Weird records: Drop the empty review(or we could remove the review with less than 5 words?)\n",
    "word_count = data.WordToken.apply(lambda x: len(x))\n",
    "filter_count = (word_count >= 1)\n",
    "data = data[filter_count]\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization: Convert the terms with different representations into the original.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [wnl.lemmatize(w) for w in text]\n",
    "\n",
    "data.insert(0, 'Lemmatized',data.WordToken.apply(lemmatize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StopWordRemoval: Remove the NLTK build-in stopwords in all the records.\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wosw = data.Lemmatized.apply(lambda x:  [item for item in x if item not in stop_words] )\n",
    "data.insert(0, 'preprocessing_finished',wosw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('hotelreviews.csv',sep=',')\n",
    "df = pd.read_csv('hotelreviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', 1, inplace =True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW TFIDF with unigram/ uni-bigram\n",
    "def dum(doc):\n",
    "    return doc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', \n",
    "                             ngram_range = (1,1),     #多1%   \n",
    "                             tokenizer = dum, \n",
    "                             preprocessor = dum,\n",
    "                             min_df = 5000)   #5000\n",
    "x = vectorizer.fit_transform(df.preprocessing_finished)\n",
    "vec = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "x_ = vec\n",
    "lbl_ = le.fit_transform(df['TripStyle'])   # 0 for Business, 1 for Leisure trip\n",
    "\n",
    "#Split them into train/test set, randomly with the test size 0.33 \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, lbl_train, lbl_test = train_test_split(x_, lbl_ , test_size = 0.33, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(lbl_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_train2 = lbl_train\n",
    "lbl_test2 = lbl_test\n",
    "print(lbl_train2.shape)\n",
    "print(lbl_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(x_train, lbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, lbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils  # 用來後續將 label 標籤轉為 one-hot-encoding \n",
    "# 建立簡單的線性執行的模型\n",
    "model = Sequential()\n",
    "# Add Input layer, 隱藏層(hidden layer) 有 256個輸出變數\n",
    "model.add(Dense(units=256, input_dim=300, kernel_initializer='normal', activation='relu')) \n",
    "# Add output layer\n",
    "model.add(Dense(units=2, kernel_initializer='normal', activation='softmax'))\n",
    "print(model.summary())\n",
    "# 編譯: 選擇損失函數、優化方法及成效衡量方式\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "lbl_train = np_utils.to_categorical(lbl_train) \n",
    "lbl_test = np_utils.to_categorical(lbl_test)\n",
    "# 進行訓練\n",
    "model.fit(x=x_train, y=lbl_train, validation_split=0.2, epochs=10, batch_size=64, verbose=2)\n",
    "scores = model.evaluate(x_test, lbl_test)\n",
    "print('test loss:', scores[0])\n",
    "print('test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function:\n",
    "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, accuracy_score, classification_report\n",
    "import imblearn\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ref: https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used\n",
    "def evaluating(test, pred, ax=object):\n",
    "  \n",
    "    print('accuracy:',accuracy_score(test, pred))\n",
    "    print('\\n')\n",
    "    print( classification_report_imbalanced(test, pred))    \n",
    "    print('\\n')\n",
    "    print ( confusion_matrix(test, pred))\n",
    "    precision, recall, threshold = precision_recall_curve(test, pred)\n",
    "\n",
    "    ax.step(recall, precision, color='b', alpha=1, where='post')\n",
    "    ax.fill_between(recall, precision, step='post', alpha=0.5, color='b')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title('Precision-Recall curve')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for test set(NB)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "pred_gnb = clf_gnb.predict(x_test)\n",
    "evaluating(lbl_test, pred_gnb, ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict classes for test set(LogisticR)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "y_pred = lr.predict(x_test)\n",
    "evaluating(lbl_test, y_pred, ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict classes for test set(DNN model)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "pred = model.predict_classes(x_test)\n",
    "evaluating(lbl_test2, pred, ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
